# 1. 介绍风控做什么的

风控决策引擎主要是提供一种可以方便的完成风控场景对接的一种能力，通过提供的可视化的配置界面系统，在系统中能够让不同业务独立配置他们的风控事件，以及事件对应的策略, 并且可以动态变更策略，可以方便的对风控事件进行管理。同时提供了策略的运行监控查询功能，方便策略维护同学分析以及观察。

使用流程也比较简单：

1. 如果是新事件，只需要在界面系统创建一个新的事件，然后在事件下面完成响应的配置即可。然后调用风控提供的 同步/异步决策接口，传入事件中约定的参数即可。
2. 如果是已有的事件，只需要在界面系统中该事件下面动态变更策略相关配置即可。然后如果有新的参数，只需要修改一下参数传递。如果没有的话，代码也不需要修改

**如果追问更多的细节**

可视化界面系统中我们将风控事件整体做了抽象，主要分为

1. 事件code
2. 事件下的 原生参数结构化配置：主要是用于管理业务方传递的业务参数的类型，用于后续引擎执行的时候对业务方参数进行解析
3. 事件下的 function  参数： 主要用于使用
   1. 特征参数 ： 用于获取，特征系统的数据依赖
   2. 脚本：用于对传递过来的原生参数做加工 或使用原生参数作为请求参数，获取其他数据源的数据
4. 事件下的 策略
   1. all and ： 配置的条件必须全部满足
   2. at least one ： 配置条件任意满足
   3. custom ： 可对配置条件做筛选 组成条件表达式。
   4. 树形结构 ： 使用各项条件作为数的节点做条件编排
5. 策略命中后返回值/ 执行的 action

# 2. 介绍系统怎么搭建的, 介绍一下里面用到的技术，分别在什么场景下面

系统使用 springboot 搭建的。

1. 远程服务调用用到了: double， springCloud
2. 事件以及事件下的策略等数据的配置使用 Mysql 作为存储
3. 使用 kakfa 消息收集事件的策略决策数据。用于生成图表数据，或者查询决策数据等场景
4. 使用了 aviator 这个适用于java的 轻量+高性能的 表达式求值引擎，来完成决策条件的判断。
5. ES 用来存储事件的决策结果数据，主要是用在我们界面系统的，决策详情查询业务中，因为每天的决策数据量很大，需要查询的时候 使用 mysql 不太合适。 而 ES 能够提供一个比较好的查询能力以及性能，数据这块我们目前是按天完成数据分片的。
6. 限流这块使用的 sentinel 来完成。

**aviator：**是一个适用于 java的 轻量+高性能的表达式求值引擎。它能够解析并计算各种类型的表达式：数学表达式，逻辑表达式，条件表达式等，并完成求值工作。并且支持表达式符号扩展。可以方便的通过它提供的扩展点完成这个扩展。同时它提供了易于使用的API，能够很方便的额将表达式求值功能集成到我们应用程序中。

# 3. 我在其中承担的角色

承担的角色：在组内承担的是主导整体设计的角色 (因为 组长他们去bn比较早,手里的bnb也没卖在那波大行情中,财务自由了,处于佛系阶段 )， 完成了这个系统从0-1的构建，以及从1-多的设计与构建

# 4. 解决了什么问题，给公司带来的价值

**描述老的系统在新的发展的需求下的问题**

老的那套风控决策是，基于coding来完成了，但是随着公司发展，以及各国对交易所的合规的越来越严格的要求。在去年开始公司顶层战略是要做全球第一的合规的交易所。那么要做好合规，以及对应不同国家定制不同的风控策略，这种场景下coding的那套就不在适用与未来的发展了，它不能很好的应对日益增长的策略需求，以及策略可能需要在过程中频繁调整的需求。

**解决方案和目标**

基于这样的需求，所以迫切的需要一个能够快速的完成新的风控业务对接，以及能够对已经运行的业务完成快速动态调整的这样的一个系统。

**初期目标是先实现**，界面化配置以及基础的 事件创建，事件中的字段定义（原生字段，特征字段，function 字段），以及风控策略的条件配置，以及实现了简单的策略决策类型： all and , at least one , custom 三种类型。策略可以自定义返回值。可配置策略命中后需要执行的 action .

**后面:** 我们又实现了，

1. 决策详情的查询： 提供一个定位线程决策结果的查询界面。方便定位问题以及查询决策详情

   1. kafak , es

2. 决策详情报表 ： 方便分析师查看策略的命中情况（命中数，命中率，命中多少人等）

   1. kafak, es,  mysql

3. 多策略的配置，以及最终决策结果的处理： 业务方希望将决策条件分到多个策略中方便管理。因为这个的原因我们也需要，增加对策略的结果的决策类型，同样的我们做了 all and , at least one , custom  这几个基础类型，未来还会增加树形决策

4. function 字段的 test 功能: 测试写的脚本是否符合预期。

5. 异步决策能力 ： 提供异步的风控决策能力，承接一些不需要实时处理以及对耗时不敏感的业务。并且将它的流量和同步引擎分离，让同步引擎能够专注处理同步的业务，也能保证他的运行更加稳定。

6. 策略条件的树形决策 : 实际的使用中，一些业务遇到了决策耗时不符合预期的问题，我们发现是因为早期的实现是将所需要依赖的字段在执行策略之前，全部进行了清洗，虽然使用了多线程，然后因为其中存在一个比较耗时的依赖，导致平均的决策耗时一直不理想。

   比如: 我们有一个风控业务场景,会设计到链上数据，数据业务方说因为链上数据很多都要请求链上的接口实时计算拿到数据,所以给到的指标是平均时间只能到500ms . 如果我们这个业务场景,每个请求都去查这个数据的话耗时最低都是 500 ms。是没办法接受的.

   因为这一点，考虑到策略中其实有很多耗时特别低的依赖是参与决策的。耗时高的依赖并不多。所以我们在想如果能实现阻断的方式完成决策，那么就不需要再执行策略之前完成所有数据的清晰了，只需要按需清洗数据即可，所以就做了树形的决策类型，让分析师将条件按照树形结构来配置，然后将耗时低的放在树的上层。上线后发现效果相当的好。

7. 策略变更的上线流程 : 为了避免一些运行中的策略，变更的时候由于一些不可预期原因的配置错误，导致线上事故。

   1. 试跑
   2. 审核
   3. 灰度
   4. 上线

8. 策略的熔断/告警功能 : 为了监控运行中的策略，避免再发生问题的时候引发比较大的线上事故，当出现不符合分析师配置的条件的时候进行熔断/告警。

9. 策略命中情况监控（基于正态分布计算得到的 标准差来对比  3sigmar 或其他n sigmar 的条件判断是否需要做告警）

**业务价值：**这套系统的实现，以及最终的效果也是达到了我们预期的目标，能够很好的完成风控场景的对接。因为决策引擎本身是无状态的，也可以根据业务请求量的变化动态的扩容缩容。并且为了保证策略运行的稳定性，安全性。我们还提供了策略上线的一套完整的上线流程功能，以及策略运行的熔断/告警功能。



# 5. 沉淀了哪些经验

项目管理：将任务划分好,做好优先级区分,然后分配任务

# 容灾，高可用怎么做的

**系统的熔断，可用性方案**

1. 应对突发流量这块我们有做**限流**。QA 环境有复制现线上的策略，测试同学会模拟请求做压测。根据这个结果我们会做线程。**并且会发出告警通知到服务负责的同学。遇到该类问题会优先操作扩容。将流量先吃下来**。如果是异常流畅再上层 Get way 会有安全的同学做拦截。(异常流量拦截（同源IP），静态内容缓存（CDN）,强化用户认证等。)

   使用的 sentinel 做限流，使用的默认策略，流量满了直接拒绝（[滑动窗口算法](../分布式/限流降级熔断/sentinel.md#滑动窗口算法)）, **降级逻辑会给到一个特定的 Code, 由业务方根据他们的业务场景自行决定在该场景下，他们是拦截重试还是直接通过**

2. **其他内存, cpu 问题等 也都是通过告警来处理，目前 BN的运维还没有提供自动扩容的功能**

**业务上的可用性，容灾方案**

2. **除此之外，我们目前还完成了对核心风控场景的 迷你规则引擎功能的支持，会提供一个sdk , 给到业务方里面是我们配置的该业务场景的最小化策略，根据使用方在启动的时候传递的业务场景做最小化的策略初始化。**

   这样在风控服务异常的时候，业务方的风控策略可以走该最小化风控策略，作用是为了提高我们的风控的基础可用

4. **除此之外还有计划中的策略降级功能：流量过高的场景，也会考虑到优先核心业务核心策略，我们会降级非黑心策略，直接让其失效。这个功能需要涉及到对事件和策略做打标。根据标签来控制非核心策略的降级。**

# QPS

同步风控接口主站的QPS 不到10000， 平均耗时 300ms ， 目前是 40台机器，机器少了会有内存问题：一个请求的上下文数据，一些结构数据。还有机器本身使用的本地策略缓存（在切换 aviator 期间很多策略还会存在两类缓存）都会占用不少内存。所以需要更多机器，同时这样也可以避免一些突发流量。

异步风控的qps 差不多在30000， 我们使用了 MQ来接受请求。好处是便于实现异步，同时可以做流量削峰（该场景对执行效率要求不高，并且也没有顺序性要求）然后就能按照我们的当前的消费能力去做消费了。如果消息积压太多我们还可以扩展机器+partition， 并且可以动态调整消费逻辑所使用的线程池参数加速消费

# 性能瓶颈点

**决策引擎的运作逻辑：**

在维护界面，定义 eventCode ,然后按照和业务方的约定，在维护节点配置参数的数据结构（原生参数）。最后配置规则策略

以及规则策略的结果处理逻辑（all and , at least one , 以及树）

在完成配置之后，在规则接口被调用的时候 会走到执行流程：

1. 将所有请求中的原生参数解析成约定的类型

2. 获取解析到的此策略需要依赖脚本/远程调用的依赖参数
   1. all and / atleast one 获取解析到的此策略需要依赖脚本/远程调用的依赖参数，使用多线程完成参数的清洗
      1. at least one 其实可以根据依赖参数中对远程参数的依赖情况（远程依赖越多的排在链表的越后面），将其解析成一个单链来执行的（将条件中的原生参数/非远程调用的参数作为链的前置节点），这样可实现阻断提高执行效率，通过前置节点做好过滤，避免后续需要执行的远程依赖的调用。
   
      2. all and 也是类似的。
      
         不过目前我们没有遇到对这块有很大诉求的问题所以暂时还没做这块的优化
      
   2. 树形结构（复杂策略）：会按照树进入的节点一层获取依赖列表，然后进行数据清晰，可以避免清晰大量数据导致性能收到影响,
   
      并且在配置的时候原生字段，内部的特征，名单数据依赖优先在上层做判断（因为我们会保证他们的查询效率，并且遇到问题能及时优化处理）。其他团队的远程数据依赖往树的下层配置。这样依赖上层可以快速的做到不错的过滤效果。提高整个事件的决策效率。
   
 	3. 根据其所选的最终结构来判断事件是否命中
             	1. all and 配置的所有策略都需要命中
             	2. at least one 任意一个策略命中则事件命中
             	3. 树性结构：
                     	1. 普通树结构：走到树的叶子节点，且命中则命中

**性能瓶颈&解决方案**

1. 一个是我们查询的时候会设计到查询三方的接口，或者特征数据，名单服务 需要他们解决接口的耗时问题，与此同时需要尽量采取多线程去对多个这类依赖完成请求（因为这里是IO密集长江。这样可以提升效率）

2. 复杂的策略，因为依赖远程调用过多，如果都清晰出来耗时会比较高，并且不能保证所有远程调用的额接口性能都很好。

   所以这时候可以让分析师使用 树形结构的决策。这样可以实现阻断。并不是清洗所有数据（不需要远程调用的策略优先在树的上层做判断），原生字段，内部的特征，名单数据依赖优先在上层做判断（因为我们会保证他们的查询效率，并且遇到问题能及时优化处理）。其他团队的远程数据依赖往树的下层配置。这样依赖上层可以快速的做到不错的过滤效果。提高整个事件的决策效率。

# 怎么保证服务每次发布的时候不造成请求耗时的尖刺

启动的时候拉取策略数据，并将策略预加载后，存放到缓存之后，再暴露服务

怎么实现的？

使用了类的 初始化后的扩展点 @PostConstruct 

目前的配置数据并没有特别大，服务启动的时候查询db，耗时并不严重。目前还未遇到启动特别慢的时候。



计划未来如果配置越来越多，影响启动耗时的时候，会将数据存放到redis中。启动的时候直接读取 redis . 

1. 在维护系统做数据变更的时候，先更新redis,然后再通知 redis channel 做变更。 

2. 定时任务拉取这块怎么实现在 redis 拉取增量: 将数据的key 按照修改时间存放到 sort set 中，以及将key对应的数据存放到 普通的key value 数据结构中

   每次拉取数据的时候拉取大于这个变更时间的数据，获取到这一批key ，然后再使用这些 key去获取变更数据完成 local cache 的更新。这样做也能避免 sort set 的key对应的 value数据太大形成大key, 服务其来初次启动的时候拉取这个数据倒是耗时过高。阻塞其他请求。

   其实还有一个问题那就是 删除的策略怎么办，其实目前我们是没有删除策略的之后设置策略失效，这事为了让分析师能够查找这类策略，复用策略中的一些配置，其次失效的策略，也可能会有重新启用的时候。

   

# 策略变更的数据一致性怎么保障的

数据保存在系统的 local cache 中。目前没有使用到 redis cache 

1. 我们会在数据变更后通过 redis channel 来完成通知。

> redis channel 的实现原理：
>
> redis 会为了订阅的chanel 维护一个表，里面存放了对应的客户端id，以及对应的 ip port等信息. 当有数据变更的时候会通知这个客户端。

2. 系统会有一个30s定时拉取增量变更数据的job. 保证在 redis chanel 推送失败的时候可以完成配属数据的最终一致性。

# 试跑阶段的细节

试跑的数据都是在 正式规则运行完之后使用单独的线程执行，不会对业务的主流程执行造成影响，于此同时会将试跑的

规则的命中信息通过kafak 发送服务端，然后消费端落地该数据。数据中记录了这次请求在正式的规则和修改后的这个试跑的规则的各自的命中情况，同时也会记录该正式规则的版本以及试跑规则的id唯一标识，用于后续查看命中详情的时候可以通过这些信息拿到规则的结构数据，在页面构造成一个直观的展示图。

这里需要强调的是，能够对比新老规则命中情况的场景只有 A -> A` 的场景, 如果是新增则没有对比一说， 同样的如果没有修改规则的条件，只是修改了规则命中后执行的action. 那么则不会有试跑这个阶段. action 的执行在创建 action的时候会有 test功能。确保能正常使用才可以绑定到规则上使用。

# 规则熔断/告警

熔断/告警的数据源是相同的区别只是 熔断相比告警需要多执行一个动作，那么就是将正在运行的规则熔断掉并且需要支持规则的自动恢复：怎么实现的呢，每15s检查一次是否满足熔断/告警配置的条件，如果满足就会发出通知。管理后台的服务端收到通知之后就会发送告警信息，并拨打配置中的电话进行通知。如果是熔断那么还会发送一个redis channel 消息。规则引擎执行端收到这个消息之后会设置一个熔断缓存，缓存时间20s. 如果在后续的检查中没有触发条件。则20s后该熔断缓存就是失效，规则也就恢复了。如果持续性检查到满足熔断条件就会持续发送消息，这个缓存就会一直存在，也就是说规则会在这个时间段内失效。



数据收集使用的是中间件提供的监控组件：时序数据库（普鲁米修斯），该组件会30s拉依次埋点数据。也就是说他支持我们配置分钟级别以上的判断条件。



怎么保证缓存一致性。变更通过 redis chanel 做通知刷新。 以及定时任务拉去 updateTiem > cache last update time 的数据。查不到数据不会走db做查询。数据缓存永久（当前的策略配置数据的体量占用的内存目前对系统还没有压力），服务启动的时候会加载所有缓存。



